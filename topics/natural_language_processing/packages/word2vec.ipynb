{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC5soMYPdiZy"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmpu7upcve"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UniquifyAI/uniquify-ai-training/blob/main/topics/natural_language_processing/packages/word2vec.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUxpl95de0bH"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this notebook, we will explore how to use Word2Vec to create word embeddings from a given text corpus. Word2Vec learns vector representations of words in such a way that words that are semantically similar have similar vector representations. These embeddings can then be used for tasks such as word similarity, analogy solving, and more.\n",
        "\n",
        "Word2Vec is a popular method for representing words as vectors in a continuous vector space, capturing semantic meanings, and relationships between words based on their context in large text corpora. Developed by Tomas Mikolov and others at Google in 2013, Word2Vec transforms words into dense vectors of fixed size, where semantically similar words are mapped to nearby points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Skip-gram Model**\n",
        "\n",
        "The **Skip-gram model** is one of the two primary models in Word2Vec (the other being CBOW). The objective of the skip-gram model is to predict the surrounding context words given a target word. It works by sliding a window over a sequence of words in a text and predicting which words are likely to occur near the target word.\n",
        "\n",
        "#### **How It Works:**\n",
        "\n",
        "- Given a target word $w_t$, the skip-gram model tries to predict the context words $w_{t-k}, w_{t+k}$ within a window of size $k$.\n",
        "- For example, if the sentence is “The cat sits on the mat,” and the target word is \"cat,\" the model would try to predict the words \"The,\" \"sits,\" and \"on\" based on \"cat.\"\n",
        "\n",
        "#### **Mathematical Formulation:**\n",
        "\n",
        "The goal of the skip-gram model is to maximize the probability of the context words given the target word:\n",
        "\n",
        "$$\n",
        "P(w_{context}|w_{target}) = \\prod_{-k \\leq j \\leq k, j \\neq 0} P(w_{t+j} | w_t)\n",
        "$$\n",
        "\n",
        "Where $w_{context}$ are the words surrounding the target word $w_t$ in a context window of size $2k$.\n",
        "\n",
        "#### **Architecture:**\n",
        "\n",
        "- **Input Layer**: The input to the model is the one-hot vector representing the target word.\n",
        "- **Hidden Layer**: A dense layer with a smaller dimensionality (embedding size) that transforms the high-dimensional one-hot encoding into a dense vector (word embedding).\n",
        "- **Output Layer**: The output layer computes the probability of each word in the vocabulary appearing as a context word given the target word.\n",
        "\n",
        "The model uses **softmax** as the activation function in the output layer to predict the probabilities for each context word."
      ],
      "metadata": {
        "id": "E-7zbpmRUJBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Continuous Bag of Words (CBOW)**\n",
        "\n",
        "The **CBOW model** is the reverse of the skip-gram model. Instead of predicting the context words given a target word, CBOW predicts the target word given its surrounding context words.\n",
        "\n",
        "#### **How It Works:**\n",
        "\n",
        "- For example, given the words \"The,\" \"cat,\" and \"on,\" the CBOW model tries to predict the target word \"sits.\"\n",
        "- It takes the average of the word embeddings of the context words and uses that to predict the target word.\n",
        "\n",
        "#### **Mathematical Formulation:**\n",
        "\n",
        "The CBOW model tries to maximize the probability of the target word given the surrounding context words:\n",
        "\n",
        "$$\n",
        "P(w_{t} | w_{t-k}, \\dots, w_{t+k})\n",
        "$$\n",
        "\n",
        "Where $w_t$ is the target word and $w_{t-k}, \\dots, w_{t+k}$ are the context words.\n",
        "\n",
        "#### **Architecture:**\n",
        "\n",
        "- **Input Layer**: The input consists of the one-hot vectors of the context words.\n",
        "- **Hidden Layer**: A dense layer transforms the context vectors into a fixed-size vector by averaging the embeddings of the context words.\n",
        "- **Output Layer**: Similar to the skip-gram model, the output layer applies a softmax to predict the target word based on the context."
      ],
      "metadata": {
        "id": "lAihZ8-SURVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word Embeddings**\n",
        "\n",
        "Both the skip-gram and CBOW models produce **word embeddings**, which are dense, low-dimensional representations of words. These embeddings capture semantic relationships between words, so words with similar meanings tend to have similar embeddings.\n",
        "\n",
        "For example, the words \"king\" and \"queen\" would have embeddings that are close in the vector space, and we can even observe patterns like:\n",
        "\n",
        "$$\n",
        "\\text{vector(\"king\")} - \\text{vector(\"man\")} + \\text{vector(\"woman\")} \\approx \\text{vector(\"queen\")}\n",
        "$$\n",
        "\n",
        "### **Negative Sampling**\n",
        "\n",
        "To make training more efficient, Word2Vec uses **negative sampling**, a technique that modifies the softmax function to consider only a small sample of negative examples (words that do not appear in the context) instead of the entire vocabulary. This reduces the computational cost, making Word2Vec suitable for large-scale text data.\n",
        "\n",
        "### **Applications of Word2Vec**\n",
        "\n",
        "- **Semantic Similarity**: Word2Vec embeddings can be used to measure the similarity between words based on their contexts.\n",
        "- **Analogy Solving**: Word2Vec can capture word relationships like “king is to queen as man is to woman.”\n",
        "- **Pre-training for NLP Models**: The learned word embeddings can be used to initialize word vectors in other natural language processing tasks, such as text classification, machine translation, and more."
      ],
      "metadata": {
        "id": "yEyuOzWCUW-y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLTGpjn9e7wj"
      },
      "source": [
        "### Objective\n",
        "The steps performed include:\n",
        "\n",
        "- Learn word embeddings from a text corpus using Word2Vec.\n",
        "- Understand the theory behind Word2Vec (Skip-gram and Continuous Bag of Words models).\n",
        "- Visualize the learned word embeddings using dimensionality reduction techniques like PCA or t-SNE.\n",
        "- Use the learned embeddings to find similarities and relationships between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuY72VQgfCdy"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Word2Vec models are typically trained on large text corpora, as they rely on the co-occurrence of words within a context window to learn word representations. Some popular datasets for training Word2Vec models include:\n",
        "- **Wikipedia Dumps**\n",
        "- **Common Crawl**\n",
        "- **Text8 Dataset** (a small compressed version of Wikipedia)\n",
        "- **Gutenberg Project Texts**\n",
        "\n",
        "For the purpose of this notebook, we'll use a smaller dataset to illustrate the process of loading and preparing text data for Word2Vec. You can either use a pre-existing dataset or scrape your own data from websites, books, or any textual resources.\n",
        "\n",
        "In this notebook, we'll use the **text8 dataset**, which is commonly used for training word embedding models. It contains about 17 million words extracted from Wikipedia and is freely available for download.\n",
        "\n",
        "If you want to use other datasets, make sure they are in plain text format. Additionally, we will need to preprocess the text, including:\n",
        "- Lowercasing all words\n",
        "- Removing punctuation\n",
        "- Tokenizing the text into words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYWfPJghm0Z"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyPUciG7fKfN"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB9zxJ5MfRjg"
      },
      "outputs": [],
      "source": [
        "%pip install -q gensim \\\n",
        "    numpy \\\n",
        "    matplotlib \\\n",
        "    requests \\\n",
        "    scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeWOwLegim00"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Z6psQqippI"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import gensim\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.word2vec import Text8Corpus\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqGp6COKj1xq"
      },
      "source": [
        "### Word2Vec Training on Text8 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiDXxMfspcvj"
      },
      "source": [
        "### Load and Preprocess the Dataset\n",
        "We will load and preprocess the Text8 dataset, as we did in the Dataset section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoeUc-fIj4UK"
      },
      "outputs": [],
      "source": [
        "# Download the Text8 dataset\n",
        "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "filename = \"text8.zip\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the dataset locally\n",
        "with open(filename, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Unzip the file\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "# Load the dataset\n",
        "with open(\"text8\", \"r\") as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "\n",
        "# Preprocessing the text\n",
        "def preprocess_text(text):\n",
        "    return simple_preprocess(text)\n",
        "\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokens = preprocess_text(text_data)\n",
        "print(f\"First 20 tokens: {tokens[:20]}\")\n",
        "\n",
        "# Output total number of tokens\n",
        "print(f\"Total number of tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkG-BbT3pcvj"
      },
      "outputs": [],
      "source": [
        "# Load the Text8 dataset\n",
        "dataset = Text8Corpus(\"text8\")\n",
        "\n",
        "# Print the first sentence from the dataset\n",
        "first_sentence = next(iter(dataset))\n",
        "print(f\"First sentence: {first_sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MclBx2iVkXG3"
      },
      "source": [
        "### Train the Word2Vec Model\n",
        "Now that the data is preprocessed, we can train the Word2Vec model using the Skip-gram method. We'll define the hyperparameters such as embedding size, window size, and number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugUiNKdak9Kz"
      },
      "outputs": [],
      "source": [
        "# Define model hyperparameters\n",
        "embedding_size = 100  # Dimensionality of word vectors\n",
        "window_size = 5  # Context window size\n",
        "min_count = 5  # Ignore words with low frequency\n",
        "workers = 4  # Number of worker threads for training\n",
        "\n",
        "# Initialize and train the Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=dataset,\n",
        "    vector_size=embedding_size,\n",
        "    window=window_size,\n",
        "    min_count=min_count,\n",
        "    workers=workers,\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"word2vec_text8.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4cI3UU1pcvk"
      },
      "source": [
        "The model is trained on the dataset, and now we have word embeddings for each word in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl5BafHrpcvk"
      },
      "source": [
        "### Visualizing Word Embeddings\n",
        "To understand how Word2Vec captures the relationships between words, we can visualize the word embeddings using t-SNE, which reduces the high-dimensional word vectors into two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t5ChjhOpcvk"
      },
      "outputs": [],
      "source": [
        "# Extract word vectors from the model\n",
        "word_vectors = model.wv\n",
        "words = list(word_vectors.index_to_key)[:500]  # Visualize only top 500 words\n",
        "\n",
        "# Get the vectors for the selected words\n",
        "word_vecs = np.array([word_vectors[word] for word in words])\n",
        "\n",
        "# Use t-SNE to reduce dimensionality\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "reduced_vecs = tsne.fit_transform(word_vecs)\n",
        "\n",
        "# Plot the t-SNE projection\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(reduced_vecs[:, 0], reduced_vecs[:, 1])\n",
        "\n",
        "# Annotate points with word labels\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(reduced_vecs[i, 0], reduced_vecs[i, 1]))\n",
        "\n",
        "plt.title(\"Word Embeddings Visualization using t-SNE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyxLrzXlpcvk"
      },
      "source": [
        "### Inference with Word2Vec\n",
        "\n",
        "After training the Word2Vec model, we can now use it to perform various tasks such as:\n",
        "- **Finding similar words** based on cosine similarity of their embeddings.\n",
        "- **Word analogies**, which help us understand the semantic relationships between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEs_mOeHpcvk"
      },
      "source": [
        "#### Finding Similar Words\n",
        "\n",
        "We can use the trained Word2Vec model to find words that are similar to a given word, based on the learned word vectors. The similarity is typically calculated using **cosine similarity** between word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJxUEByspcvk"
      },
      "outputs": [],
      "source": [
        "# Load the saved Word2Vec model\n",
        "model = Word2Vec.load(\"word2vec_text8.model\")\n",
        "\n",
        "# Find words similar to 'king'\n",
        "similar_words = model.wv.most_similar(\"king\", topn=5)\n",
        "print(f\"Words most similar to 'king':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtloZRrOpcvl"
      },
      "source": [
        "#### Word Analogies\n",
        "One of the most famous features of Word2Vec is the ability to perform word analogies. For example, the relationship between \"king\" and \"queen\" is similar to the relationship between \"man\" and \"woman\". We can find the word that completes the analogy \"king is to man as queen is to ...\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOeusX83pcvl"
      },
      "outputs": [],
      "source": [
        "# Performing word analogy: 'king' is to 'man' as 'queen' is to ?\n",
        "result = model.wv.most_similar(positive=[\"queen\", \"man\"], negative=[\"king\"], topn=1)\n",
        "print(f\"'King' is to 'man' as 'queen' is to: {result[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCpoJM5Vpcvl"
      },
      "source": [
        "#### Finding the Similarity between Two Words\n",
        "We can also calculate the similarity between two words to understand how closely related they are in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miptTRXSpcvl"
      },
      "outputs": [],
      "source": [
        "# Calculate similarity between \"king\" and \"queen\"\n",
        "similarity_score = model.wv.similarity(\"king\", \"queen\")\n",
        "print(f\"Similarity between 'king' and 'queen': {similarity_score}\")\n",
        "\n",
        "# Calculate similarity between \"king\" and \"car\"\n",
        "similarity_score = model.wv.similarity(\"king\", \"car\")\n",
        "print(f\"Similarity between 'king' and 'car': {similarity_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quizzes"
      ],
      "metadata": {
        "id": "-oxpYbivVY4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is Word2Vec used for?**\n",
        "* a. Image recognition\n",
        "* b. Learning word embeddings\n",
        "* c. Time series analysis\n",
        "* d. Database management\n",
        "\n",
        "<details>\n",
        "<summary>Answers</summary>\n",
        "<p>b. Learning word embeddings</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "UYBSClNvVcyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **What are word embeddings?**\n",
        "\n",
        "* a. Images representing words\n",
        "* b. Numerical vectors representing words\n",
        "* c. Audio files representing words\n",
        "* d. HTML code representing words\n",
        "\n",
        "<details>\n",
        "  <summary>Answer</summary>\n",
        "  <p>b. Numerical vectors representing words</p>\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "_RkThtekV-bB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **What are the two main architectures of Word2Vec?**\n",
        "\n",
        "* a. CNN and RNN\n",
        "* b. CBOW and Skip-gram\n",
        "* c. LSTM and GRU\n",
        "* d. SVM and Naive Bayes\n",
        "\n",
        "<details>\n",
        "  <summary>Answer</summary>\n",
        "  <p>b. CBOW and Skip-gram</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "L-MUyWh-WPKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **What does CBOW stand for?**\n",
        "\n",
        "* a. Continuous Binary Output Words\n",
        "* b. Continuous Bag-of-Words\n",
        "* c. Contextual Bit-Oriented Words\n",
        "* d. Cross-Boundary Output Words\n",
        "\n",
        "<details>\n",
        "  <summary>Answer</summary>\n",
        "  <p>b. Continuous Bag-of-Words</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "WKAZKAecWoKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **How does Word2Vec capture the semantic meaning of words?**\n",
        "\n",
        "* a. By counting the frequency of words\n",
        "* b. By analyzing the grammatical structure of sentences\n",
        "* c. By learning the context in which words appear\n",
        "* d. By using a dictionary of synonyms\n",
        "\n",
        "<details>\n",
        "  <summary>Answer</summary>\n",
        "  <p>c. By learning the context in which words appear</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "9U3f8kjPXJtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Explain how the Skip-gram model works.**\n",
        "\n",
        "<details>\n",
        "  <summary>Answer</summary>\n",
        "  <p>The Skip-gram model takes a target word as input and tries to predict the surrounding context words. It does this by training a neural network to maximize the probability of predicting the context words given the target word.</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "XNBy3vs4XjJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Explain how the CBOW model works.**\n",
        "\n",
        "<details>\n",
        "  <summary>Answer</summary>\n",
        "  <p>The CBOW model takes the surrounding context words as input and tries to predict the target word. It trains a neural network to maximize the probability of predicting the target word given the context words.</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "y-KAZxBuXsjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Learning and Resources\n",
        "\n",
        "* **Original Word2Vec Papers:**\n",
        "    * Efficient Estimation of Word Representations in Vector Space: [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)\n",
        "    * Distributed Representations of Words and Phrases and their Compositionality: [https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)\n",
        "* **Gensim Documentation:** Comprehensive documentation for the Gensim library, including Word2Vec. [https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)\n",
        "* **Stanford CS224N:** Natural Language Processing with Deep Learning. [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)\n",
        "* **Deep Learning for NLP (Goldberg):** A comprehensive textbook on deep learning for natural language processing. [https://www.morganclaypoolpublishers.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037](https://www.morganclaypoolpublishers.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037)\n",
        "* **Hugging Face Transformers:** While not Word2Vec, it's the current state of the art. [https://huggingface.co/transformers/](https://huggingface.co/transformers/)\n"
      ],
      "metadata": {
        "id": "X0Fz8VaMT3Fb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlMgw0LGiWVe"
      },
      "source": [
        "## Conclusion\n",
        "In this notebook, we explored how to use Word2Vec for learning word representations from text data. Here's a summary of the steps we followed:\n",
        "\n",
        "- **Overview**: We introduced Word2Vec and its use for generating word embeddings.\n",
        "- **Dataset**: We used the Text8 dataset, a sample of text data, and preprocessed it for training.\n",
        "- **Training**: We trained a Word2Vec model using the Gensim library, applying the Skip-gram method to learn word embeddings.\n",
        "- **Visualization**: We visualized the word embeddings using t-SNE to observe how similar words are clustered in the vector space.\n",
        "\n",
        "### Key Takeaways:\n",
        "- Word2Vec is a powerful tool for learning word embeddings from large text corpora, capturing semantic similarities between words.\n",
        "- Skip-gram and CBOW are two fundamental models used in Word2Vec, and the choice between them depends on the task and dataset.\n",
        "- Word embeddings can be used in various natural language processing tasks, such as text classification, semantic similarity, and more."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}